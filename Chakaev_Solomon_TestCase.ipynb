{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Внесенные в код правки\n",
    "\n",
    "1. В классе `Attention`:\n",
    "   - В `super()` добавил явное указание наследования от `nn.Module`\n",
    "\n",
    "2. В классе `TransformerBlock`:\n",
    "   - В `super()` добавил явное указание наследования от `nn.Module`\n",
    "\n",
    "\n",
    "3. В классе `LanguageModel`:\n",
    "   - В `super()` добавил явное указание наследования от `nn.Module`\n",
    "   - Изменил определение входного эмбеддинга: `self.tok_emb = nn.Embedding(Config.vocab_size, Config.n_embd)`. \n",
    "   - В методе `_init_weights`, внес изменения в инициализацию весов, чтобы использовать `nn.init.xavier_uniform_` для весов и `nn.init.zeros_` для смещения (если оно есть). В нейронных сетях нельзя инициализировать весы нулями.\n",
    "\n",
    "4. В инициализации `vocab` и `batch`:\n",
    "   - Добавил кавычки вокруг строк в `vocab` и `batch`\n",
    "\n",
    "5. В инициализации переменных `PAD`, `BOS`, `EOS`, и `UNK`:\n",
    "   - Добавил инициализация переменных `PAD`, `BOS`, `EOS`, и `UNK` перед использованием для корректной работы кода\n",
    "\n",
    "6. В методе `forward` класса `LanguageModel`:\n",
    "   - Заменил значение `reduction` в функции `F.cross_entropy` на `'mean'` для получения общего лосса: `loss = F.cross_entropy(logits.permute(0, 2, 1), targets, reduction='mean')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "tensor(6.9714, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import razdel\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Config:\n",
    "    n_head = 8 #является делителем размерности вложения (n_embd), поэтому меняю (32 делится на 8)\n",
    "    n_embd = 32 #меняю чтобы соответствовало MAX_LEN ниже\n",
    "    n_layer = 12\n",
    "    seq_len = 32\n",
    "    embd_pdrop = 0.5\n",
    "    resid_pdrop = 0.5\n",
    "    attn_pdrop = 0.5\n",
    "    vocab_size = 1024\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):  # <-- исправляем здесь\n",
    "        super(Attention, self).__init__()  # <-- исправляем здесь\n",
    "\n",
    "        assert Config.n_embd % Config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(Config.n_embd, Config.n_embd)\n",
    "        self.query = nn.Linear(Config.n_embd, Config.n_embd)\n",
    "        self.value = nn.Linear(Config.n_embd, Config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(Config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(Config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(Config.n_embd, Config.n_embd)\n",
    "        \n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, Config.n_head, C // Config.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, Config.n_head, C // Config.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, Config.n_head, C // Config.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):  # <-- исправляем здесь\n",
    "        super(TransformerBlock, self).__init__()  # <-- исправляем здесь\n",
    "        self.norm1 = nn.BatchNorm1d(Config.n_embd)\n",
    "        self.norm2 = nn.BatchNorm1d(Config.n_embd)\n",
    "        self.attn = Attention()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(Config.n_embd,  Config.n_embd // 16),\n",
    "            nn.Linear(Config.n_embd // 16, Config.n_embd),\n",
    "            nn.Dropout(Config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x))\n",
    "        x = self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):  # <-- исправляем здесь\n",
    "        super(LanguageModel, self).__init__()  # <-- исправляем здесь\n",
    "\n",
    "        self.tok_emb = nn.Embedding(Config.vocab_size, Config.n_embd)  # <-- исправляем здесь\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock() for _ in range(Config.n_layer)])\n",
    "        self.norm_f = nn.BatchNorm1d(Config.n_embd)\n",
    "        self.head = nn.Linear(Config.n_embd, Config.vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear)):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        b, t = idx.size()\n",
    "        assert t <= Config.seq_len, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        x = self.tok_emb(idx)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm_f(x)\n",
    "        logits = self.head(x)\n",
    "        loss = F.cross_entropy(logits.permute(0, 2, 1), targets, reduction='mean') #поменял на mean чтобы выдавался общий лосс\n",
    "        return logits, loss\n",
    "\n",
    "lm = LanguageModel()\n",
    "vocab = [\"мама\", \"компьютер\", \"мыла\", \"раму\", \"текст\", \"сгенерировал\", \"длинный\"]  # <-- исправляем здесь\n",
    "batch = [\"Мама мыла раму\", \"Компьютер сгенерировал длинный текст\"]  # <-- исправляем здесь\n",
    "\n",
    "PAD = 0 #инициализирую нужные переменные \n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3\n",
    "token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "\n",
    "batch = [[token2idx.get(token.text, 3) for token in razdel.tokenize(text.lower())] for text in batch]\n",
    "MAX_LEN = 32\n",
    "batch_input = torch.zeros((len(batch), MAX_LEN), dtype=torch.long)\n",
    "# padleft each sample with PAD to MAX_LEN\n",
    "for i, row in enumerate(batch):\n",
    "    row = torch.tensor(row)\n",
    "    batch_input[i, -len(row) - 2] = BOS\n",
    "    batch_input[i, -len(row) - 1:-1] = row\n",
    "    batch_input[i, -1] = EOS\n",
    "\n",
    "logits, loss = lm.forward(batch_input, batch_input)\n",
    "print(loss.shape)\n",
    "loss.mean().backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. В классе `Attention`:\n",
    "   - В `super()` добавил явное указание наследования от `nn.Module`\n",
    "\n",
    "2. В классе `TransformerBlock`:\n",
    "   - В `super()` добавил явное указание наследования от `nn.Module`\n",
    "\n",
    "\n",
    "3. В классе `LanguageModel`:\n",
    "   - В `super()` добавил явное указание наследования от `nn.Module`\n",
    "   - Изменил определение входного эмбеддинга: `self.tok_emb = nn.Embedding(Config.vocab_size, Config.n_embd)`. \n",
    "   - В методе `_init_weights`, внес изменения в инициализацию весов, чтобы использовать `nn.init.xavier_uniform_` для весов и `nn.init.zeros_` для смещения (если оно есть). В нейронных сетях нельзя инициализировать весы нулями.\n",
    "\n",
    "4. В инициализации `vocab` и `batch`:\n",
    "   - Добавил кавычки вокруг строк в `vocab` и `batch`\n",
    "\n",
    "5. В инициализации переменных `PAD`, `BOS`, `EOS`, и `UNK`:\n",
    "   - Добавил инициализация переменных `PAD`, `BOS`, `EOS`, и `UNK` перед использованием для корректной работы кода\n",
    "\n",
    "6. В методе `forward` класса `LanguageModel`:\n",
    "   - Заменил значение `reduction` в функции `F.cross_entropy` на `'mean'` для получения общего лосса: `loss = F.cross_entropy(logits.permute(0, 2, 1), targets, reduction='mean')`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
